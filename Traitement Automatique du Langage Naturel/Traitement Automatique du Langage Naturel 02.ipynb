{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Automatique du Langage Naturel 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enoncé :\n",
    "En utilisant les deux fichiers Text_data_1.txt et Text_data_2.txt donnés en TP, donnez le code source\n",
    "en python en suivant les instructions suivantes :\n",
    "\n",
    "1. Importez spaCy et chargez le modèle en_core_web_sm qu'on nommera nlp\n",
    "\n",
    "2. Importez matplotlib.pyplot as plt afin d'aficher des figures\n",
    "\n",
    "3. Définissez une petite fonction nommée get_word_vectors qui retourne la forme vectorielle d'une liste de mots donnés en entrée, ex : utilisez nlp(word).vector pour avoir la forme vectorielle du mot word.\n",
    "\n",
    "4. En suivant les étapes données dans la fiche de TP1, récupérez manuellement la liste des quarante mots les plus utilisés dans chacun des deux fichiers et mettez-les dans deux listes difiérentes.\n",
    "\n",
    "5. Les vecteurs sont donnés en 300 dimensions, pour visualiser le résultat souhaité en deux dimensions, utilisez le PCA (Principal Component Analysis). Pour cela, utilisez simplement la suite d'instructions :\n",
    "\n",
    " - from sklearn.decomposition import PCA\n",
    "\n",
    " - pca = PCA(n_components = 2)\n",
    "\n",
    " - pca.fit(get_word_vectors(words1)) (words1 : liste des mots du fichier 1 par exemple)\n",
    "\n",
    " - word_vecs_2d_1 = pca.transform(get_word_vectors(words1))\n",
    "\n",
    "6. A ce niveau, vous avez donc deux listes contenants chacune les quarante mots les plus utilisés dans chaque fichier, et ceci, sous la forme d'un vecteur à deux dimensions. Donnez le code source qui puisse aficher les deux figures suivantes :\n",
    "\n",
    " - Figure 1 : présente les coordonnées de chacun des mots de chacun des deux fichiers (utilisez deux symboles différents pour chaque liste de mots)\n",
    "\n",
    " - Figure 2 : présente les mots correspondant à chacun des deux fichiers (utilisez deux couleurs différentes pour chaque liste de mots)\n",
    "\n",
    "Question :\n",
    "- Que pensez vous des résultats obtenus à l'oeil nu (concernant la similarité des deux fichiers) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improtation des bibliotheques nécéssaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import numpy as np\n",
    "import gensim \n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier1 = list();\n",
    "fichier2 = list();\n",
    "plx = list();\n",
    "ply = list();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction de vectorisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(word):\n",
    "    ab = list();\n",
    "    for token in word:\n",
    "        ab.append(nlp(token).vector)\n",
    "    return ab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des 40 mots les plus utilisé dans le fichier 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des 40 mots les plus repeter dans le fichier text : ['China', 'air', 'pollution', 'coal', 'quality', 'power', 'year', 'and', 'but', 'global', 'government', 'solar', 'datum', 'country', 'actually', 'energy', 'PM2.5', 'US', 'statistic', 'this', 'clean', 'so', 'environmental', 'course', 'consumption', 'carbon', 'one', 'good', 'people', 'chinese', 'environment', 'policy', 'like', 'fact', 'world', 'in', 'reduction', 'climate', 'emission', 'the']\n"
     ]
    }
   ],
   "source": [
    "file= open(\"Text_01.txt\",\"r\") # Ouvrir le fichier en lecture\n",
    "token_list = []                 # Créer une liste pour les tokens\n",
    "for line in file :\n",
    "    one_line = nlp(line)        # Faire l'analyse de chaque ligne du fichier via la fonction 'nlp'\n",
    "    token_list.extend(one_line) # Ajouter la ligne analysée à la liste 'token_list' via la fonction 'extend'\n",
    "file.close;                     # Fermer le fichier\n",
    "#for token in token_list :       # On peut par exemple afficher les lemmes  de ces tokens\n",
    "    #print(token.lemma_)    \n",
    "\n",
    "list_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS) # Récupérer la liste des stopwords\n",
    "#print(list_stopwords[:])                                   # Affichage des stopwords \n",
    "\n",
    "\n",
    "ponct = [',','.','!','?','\\n',';',':','\"','“','”','‘','(',')',\"'\",'[',']','--','...','/'] # Créer une liste contenant les symboles de ponctuation\n",
    "list_stopwords.extend(ponct)                                                              # Ajouter les ponctuations à la liste des stopwords\n",
    "#print(list_stopwords[:])                                                                  # Afficher la nouvelle liste\n",
    "\n",
    "token_list_filtred1 = [token for token in token_list if token.text not in list_stopwords] # Eliminer les tokens correspondant aux stopwords\n",
    "\n",
    "\n",
    "token_list_filtred2 = [token for token in token_list_filtred1 # Eliminer les pronoms et les verbes\n",
    "                       if token.lemma_ !='-PRON-'\n",
    "                       and token.lemma_ != '-'\n",
    "                       and token.pos_ != 'VERB'\n",
    "                      ]\n",
    "#print(token_list_filtred2)                                    # Afficher la nouvelle liste\n",
    "\n",
    "token_list_filtred2_lemma = [token.lemma_ for token in token_list_filtred2] # Récupérer tous les lèmmes de la nouvelle liste des tokens\n",
    "word_counter = {}                                                           # Créer un dicionnaire\n",
    "for word in token_list_filtred2_lemma:                                      # Compter le nombre d'apparition de chaque lèmme\n",
    "        if word in word_counter:\n",
    "            word_counter[word] += 1\n",
    "        else:\n",
    "            word_counter[word] = 1\n",
    "#print(word_counter)                                                         # Afficher le dicionnaire\n",
    "\n",
    "# Tri des lèmmes en fonction du nombre d'apparition:\n",
    "# --------\n",
    "SortedFiltreLemma=dict();    \n",
    "list_keys = []            \n",
    "for key, value in sorted(word_counter.items(), key=lambda item: item[1],reverse=True):\n",
    "    SortedFiltreLemma[key]=value\n",
    "    list_keys.append(key)\n",
    "    #print(\"%s: %s\" % (key, value))\n",
    "\n",
    "    \n",
    "list_keys_filtred1=list_keys[0:40]\n",
    "print (\"Liste des 40 mots les plus repeter dans le fichier text :\",list_keys_filtred1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation des 40 mots les plus utilisé dans le fichier 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des 40 mots les plus repeter dans le fichier text : ['energy', 'relationship', 'and', 'like', 'power', 'problem', 'but', 'coal', 'calorie', 'okay', 'question', 'so', 'plant', 'be', 'thing', 'day', 'the', 'laughter', 'human', 'well', 'electricity', 'good', 'that', 'mile', 'there', 'journalist', 'role', 'here', 'physics', 'smart', 'because', 'insignificant', 'year', 'grid', 'step', 'communication', 'beer', 'ago', 'use', 'then']\n"
     ]
    }
   ],
   "source": [
    "file= open(\"Text_02.txt\",\"r\") # Ouvrir le fichier en lecture\n",
    "token_list = []                 # Créer une liste pour les tokens\n",
    "for line in file :\n",
    "    one_line = nlp(line)        # Faire l'analyse de chaque ligne du fichier via la fonction 'nlp'\n",
    "    token_list.extend(one_line) # Ajouter la ligne analysée à la liste 'token_list' via la fonction 'extend'\n",
    "file.close;                     # Fermer le fichier\n",
    "#for token in token_list :       # On peut par exemple afficher les lemmes  de ces tokens\n",
    "    #print(token.lemma_)    \n",
    "\n",
    "list_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS) # Récupérer la liste des stopwords\n",
    "#print(list_stopwords[:])                                   # Affichage des stopwords \n",
    "\n",
    "\n",
    "ponct = [',','.','!','?','\\n',';',':','\"','“','”','‘','(',')',\"'\",'[',']','--','...','/'] # Créer une liste contenant les symboles de ponctuation\n",
    "list_stopwords.extend(ponct)                                                              # Ajouter les ponctuations à la liste des stopwords\n",
    "#print(list_stopwords[:])                                                                  # Afficher la nouvelle liste\n",
    "\n",
    "token_list_filtred1 = [token for token in token_list if token.text not in list_stopwords] # Eliminer les tokens correspondant aux stopwords\n",
    "\n",
    "\n",
    "token_list_filtred2 = [token for token in token_list_filtred1 # Eliminer les pronoms et les verbes\n",
    "                       if token.lemma_ !='-PRON-'\n",
    "                       and token.lemma_ != '-'\n",
    "                       and token.pos_ != 'VERB'\n",
    "                      ]\n",
    "#print(token_list_filtred2)                                    # Afficher la nouvelle liste\n",
    "\n",
    "token_list_filtred2_lemma = [token.lemma_ for token in token_list_filtred2] # Récupérer tous les lèmmes de la nouvelle liste des tokens\n",
    "word_counter = {}                                                           # Créer un dicionnaire\n",
    "for word in token_list_filtred2_lemma:                                      # Compter le nombre d'apparition de chaque lèmme\n",
    "        if word in word_counter:\n",
    "            word_counter[word] += 1\n",
    "        else:\n",
    "            word_counter[word] = 1\n",
    "#print(word_counter)                                                         # Afficher le dicionnaire\n",
    "\n",
    "# Tri des lèmmes en fonction du nombre d'apparition:\n",
    "# --------\n",
    "SortedFiltreLemma=dict();    \n",
    "list_keys = []            \n",
    "for key, value in sorted(word_counter.items(), key=lambda item: item[1],reverse=True):\n",
    "    SortedFiltreLemma[key]=value\n",
    "    list_keys.append(key)\n",
    "    #print(\"%s: %s\" % (key, value))\n",
    "\n",
    "    \n",
    "list_keys_filtred2=list_keys[0:40]\n",
    "print (\"Liste des 40 mots les plus repeter dans le fichier text :\",list_keys_filtred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction de dimession pour le 1er vecteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "words1 = list()\n",
    "words1 = list_keys_filtred1\n",
    "pca.fit(get_word_vectors(words1)) # (words1 : liste des mots du fichier 1)\n",
    "word_vecs_2d_1 = pca.transform(get_word_vectors(words1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction de dimession pour le 2eme vecteur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "words2 = list()\n",
    "words2 = list_keys_filtred2\n",
    "pca.fit(get_word_vectors(words2)) # (words2 : liste des mots du fichier 2)\n",
    "word_vecs_2d_2 = pca.transform(get_word_vectors(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xf95c9a8288>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(word_vecs_2d_1[:,0:1],word_vecs_2d_1[:,1:],\"rX\")\n",
    "plt.plot(word_vecs_2d_2[:,0:1],word_vecs_2d_2[:,1:],\"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reponse à la dernière question :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La similarité des deux fichier est presque parfaite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
