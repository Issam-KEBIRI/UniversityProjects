{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement, codage et reconnaissance du locuteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import dct, idct\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio,display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les pré-requis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions de conversion utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir une fréquence en mel vers une fréquence en Hz\n",
    "def Mel2Hz(mel):\n",
    "    return 700 * (np.power(10,mel/2595)-1)\n",
    "\n",
    "#convertir une fréquence en Hz vers une fréquence en Mel\n",
    "def Hz2Mel(freq):\n",
    "    return 2595 * np.log10(1+freq/700)\n",
    "\n",
    "#convertir une fréquence Hz en indice de matrice (sachant que fs --> T)\n",
    "def Freq2Ind(freq,fs,T):\n",
    "    return (freq*T/fs).astype(int)\n",
    "\n",
    "#Fenêtre de hamming\n",
    "def hamming(T):\n",
    "    t=np.arange(T)\n",
    "    return 0.54-0.46*np.cos(2*np.pi*t/(T-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie prétraitement : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions qu'on vas utilisées pour le prétraitement de la parole (filtrage du bruit, fenetrage de hamming, calcul de spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtre de préaccentuation - Augmenter les hautes fréquences\n",
    "def preaccentuation(x):\n",
    "    return x[:-1]-0.97*x[1:]\n",
    "\n",
    "\n",
    "#Fenêtre de hamming - Pondération des fenêtres\n",
    "def hamming(T):\n",
    "    t=np.arange(T)\n",
    "    return 0.54-0.46*np.cos(2*np.pi*t/(T-1))\n",
    "\n",
    "\n",
    "#calcul du spectrogram\n",
    "def spectrogram(x,T,p,window=None,fftsz=None):\n",
    "    n=len(x) #taille de x\n",
    "    m=len(np.arange(0,n-T,p)) #estimation du nombre de fenêtres\n",
    "    S=np.zeros((m,T))\n",
    "\n",
    "    if fftsz is None: fftsz=T #si vide on prend la valeure de T\n",
    "    if window is None: window=np.ones(T) #si vide on prend des 1\n",
    "    \n",
    "    #début fenêtrage - découpage du signal\n",
    "    for i in range(m):\n",
    "        S[i,:]=x[i*p:i*p+T]*window\n",
    "            \n",
    "    #Transformée de Fourier - Passage du domaine temporel vers le domaine spectral        \n",
    "    S=np.fft.fft(S,fftsz)\n",
    "    \n",
    "    amp=np.abs(S) #spectre d'amplitude\n",
    "    phase=np.angle(S) #spectre de phase\n",
    "    \n",
    "    return amp,phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie Codage : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des caractéristiques (création des filtres oreille pour ensuite pouvoir extraire les paramètres pertinentes MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cette fonction va s'en charger de créer les filtres oreilles  \n",
    "#Créer des filtres proches de ceux de l'oreille, nf: nombre de filtres à créer\n",
    "def filtresOreille(nf,T,fs):\n",
    "    freqMin=50  #Hz On définit la fréquence minimum que perçoit l'oreille\n",
    "    freqMax=min(10000, fs/2)  #Hz On définit la fréquence maximal que perçoit l'oreille \n",
    "\n",
    "    #convertir en mel\n",
    "    melMin=Hz2Mel(freqMin) # 50Hz = 77mel\n",
    "    melMax=Hz2Mel(freqMax) # 10000Hz = 3073mel\n",
    "\n",
    "    #calcul du début et la fin de chaque filtre en Mel\n",
    "    pas=(melMax-melMin)/(nf+1)  #diviser l'espace de perception de l'oreille sur le nombre de filtres\n",
    "    debut=np.arange(melMin,melMax-2*pas+1,pas) #réalisation de filtres qui se chevauchent de moitier\n",
    "    fin=debut+2*pas\n",
    "    \n",
    "    #convertir les filtres dans les fréquences (Hz)\n",
    "    debuthz=Mel2Hz(debut)\n",
    "    finhz=Mel2Hz(fin)\n",
    "\n",
    "    #convertir les fréquences en indices\n",
    "    debutI=Freq2Ind(debuthz,fs,T)  \n",
    "    finI=Freq2Ind(finhz,fs,T)\n",
    "\n",
    "    #construie les filtres \n",
    "    filtres=np.zeros((int(T/2),nf))\n",
    "    for i in range(len(debutI)):\n",
    "        filtres[debutI[i]:finI[i],i]=hamming(finI[i]-debutI[i])\n",
    "    \n",
    "    return filtres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction mfcc tiré de la dernière partie de la demonstration des MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des 13 paramètres pertinentes\n",
    "#Calcul des coefficients MFCC, data: le vecteur des données (domaine temporel), T: taille de la fenêtre \n",
    "# p: le pas , filtres: matrice des filtres de l'oreille, nc: le nombre de coefficients à garder\n",
    "def mfcc(data,filtres,T=512,p=256,nc=13):\n",
    "    amp,phase=spectrogram(preaccentuation(data),T,p,window=hamming(T))  #calcul du spectrogramme\n",
    "    amp=amp[:,:int(T/2)] #On coupe le spectre d'amplitude en 2 à cause de l'effet miroir\n",
    "    amp=np.dot(amp,filtres) #application des filtres de l'oreille \n",
    "    amp=np.log(amp) #calcul du log du spectre (convertir la multiplication en addition)\n",
    "    amp=idct(amp, norm = 'ortho') #calcul de l'idct du spectre filtré en log\n",
    "    amp=amp[:,:nc] #ne choisir que les \"nc\" premiers (réduction des données + lissage ceptral)\n",
    "    \n",
    "    return amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie Reconnaissance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debut du travail avec le Chargement du Dataset, ensuite arrive la Partie prétraitement et codage detaillé en haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Chargement du Dataset##############################################################\n",
    "chemin='C:\\\\Users\\\\pc\\Desktop\\\\Mes Documents\\\\Eddirassa\\\\MI M1\\\\S2\\Mes TPs\\\\TP TParoles\\\\Ressources\\\\free-spoken-digit-dataset-master\\\\recordings\\\\'\n",
    "locuteurs=['jackson','nicolas','theo','yweweler']\n",
    "\n",
    "\n",
    "datamfcc=[] #on va stocker les mfcc de chaque fichier audio dans cette liste\n",
    "labels=[] #on va stocker la classe réelle [0-9] de chaque fichier audio dans cette liste\n",
    "\n",
    "\n",
    "# Lecture, Normalisation et extraction des mfcc pour chaque chiffre des quatres locuteurs\n",
    "for ch in range(10):  #pour les chiffres de 0-9\n",
    "    for loc in locuteurs: #pour chaque locuteur\n",
    "        for i in range(50): #chaque locuteur a répété 50 fois le même chiffre\n",
    "            fichier=\"%d_%s_%d.wav\"%(ch,loc,i) #le nom du fichier à charger\n",
    "            \n",
    "            fs, data = wavfile.read(chemin+fichier) #lire un fichier audio \n",
    "            data=data/max(abs(data))  #le normaliser entre -1 et 1\n",
    "            T=512\n",
    "            n=256\n",
    "            nf=36\n",
    "            nc=13\n",
    "            filtres=filtresOreille(nf,T,fs) #construction des filtres\n",
    "            fichier_mfcc=mfcc(data,filtres) #calculer les mfccs\n",
    "            #Donc les parametres mfcc extrais de tt les audions sont stocké dans fichier_mfcc\n",
    "          \n",
    "            \n",
    "            datamfcc.append(fichier_mfcc) #ajouter les mfcc du fichier à la liste\n",
    "            labels.append(ch) #ajouter la classe du fichier aux labels (s'il s'agit d'un 0 ou 1 ou 2...)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mélange aléatoire du dataset et division 50% train / 50% test\n",
    "n=len(labels)  #le nombre de fichiers\n",
    "\n",
    "indices=np.arange(n)  #mélange aléatoire du dataset\n",
    "np.random.shuffle(indices)\n",
    "datamfcc=np.array(datamfcc)[indices]\n",
    "labels=np.array(labels)[indices]\n",
    "\n",
    "m=int(n*0.5)  #prendre 50% comme train et 50% comme test\n",
    "trainmfcc=datamfcc[:m] #données d'apprentissage\n",
    "trainlabels=labels[:m] #sortie désirée pour chaque donnée d'apprentissage\n",
    "testmfcc=datamfcc[m:] #données de test, ne pas utiliser qu'à la fin pour tester (calcul du taux) \n",
    "testlabels=labels[m:] #sortie désirée (réelle) de chaque donnée de test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapport détaillé :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " j'ai opté pour l'utilisation de la méthode Dtw (Dynamic Time Warping) pour la comparaison des paramètres acoustiques deja extrais par notre methode MFCC. La Déformation Dynamique Temporelle (DTW) permet de mesurer la différence entre deux signaux temporels pouvant présenter des décalages, des déformations l’un par rapport à l’autre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme Dtw construit une matrice des distances entre les 2 echantillos. Le calcul de la distance enter échantillon sera une distance euclidienne. il faut construire au préalable une matrice des distances , ayant 2 vecteurs X1 et X2 par exemples , on prend chaque point du vecteur X1 et on calcule la distance avec tous les points du vecteur X2, ainsi de suite. chaque ligne (qui représente un segment du signal) de ma matrice est constituée des 13 coéfficients MFCC (en colonne) . Le nombre de ligne de ma matrice représente ainsi le nombre de segments du signal et les colonnes représentent les 13 coéfficints MFCC retenus pour chaque segment, Ma matrice n*13 représente \"n\" echantillons, et chaque echantillon à 13 parametres. Il nous faut donc créer une distance pour comparer deux vecteurs en dimension 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et tt ce travail sera appliqueé aux audios des 04 locuteurs, c'est pour ca j'ai preferé travaillé avec le trainmfcc et le trainlabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de reconnaissance :\n",
    "def reconnaissance(f_mfcc):\n",
    "    dmin, jmin, i = np.inf, -1, 0\n",
    "    for x in trainmfcc:   #pour chaque exemple de la base d'apprentissage\n",
    "        d, _, _, _ = dtw(f_mfcc, x, dist=lambda x, y: np.sum(np.abs(x-y)))   #calculer sa distance avec notre échantillon qu'on veut reconnaitre\n",
    "        if d < dmin: #si la distance est la plus petite jusqu'à maintenant\n",
    "            dmin=d  #sauvgarder la distance la plus petite\n",
    "            jmin=i  #récupérer l'indice de l'exemple le plus proche\n",
    "       \n",
    "        i=i+1\n",
    "        \n",
    "    return trainlabels[jmin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponses correctes: 976 / 1000\n",
      "Taux de reconnaissance: 97.60%\n"
     ]
    }
   ],
   "source": [
    "###################Test du modèle avec (testmfcc et testlabel)##############################################\n",
    "reponse=[] #initialiser la liste qui va contenir la réponse de votre méthode pour chaque fichier test\n",
    "for f_mfcc in testmfcc:\n",
    "    reponse.append(reconnaissance(f_mfcc))\n",
    "    \n",
    "correct=sum(reponse==testlabels) #calcul du nombre de réponses correctes\n",
    "print('Réponses correctes: %d / %d'%(correct,len(testlabels)))\n",
    "\n",
    "taux=np.mean(reponse==testlabels) #calcul en pourcentage (taux de reconnaissance)\n",
    "print('Taux de reconnaissance: %.2f%%'%(taux*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  La reconnaissance du locuteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons etiqueter chaq'un des quatre locuteurs par une etiquete, puis re-faire presque le meme travail que la reconnaissance des chiffres, CAD choisir une méthode (Dtw par exemple) et passer par une partie trainning et une partie test pour pouvoir reconnaitre notre locuteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
